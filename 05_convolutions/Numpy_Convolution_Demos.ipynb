{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution demos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use the batch, multi-channel convolution operation implemented in Numpy (that you can find [here](../lincoln/lincoln/conv.py)) to train a small convolutional neural network to more than 90% accuracy on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# import lincoln\n",
    "# from lincoln.layers import Dense\n",
    "# from lincoln.losses import SoftmaxCrossEntropy, MeanSquaredError\n",
    "# from lincoln.optimizers import Optimizer, SGD, SGDMomentum\n",
    "# from lincoln.activations import Sigmoid, Tanh, Linear, ReLU\n",
    "# from lincoln.network import NeuralNetwork\n",
    "# from lincoln.train import Trainer\n",
    "# from lincoln.utils import mnist\n",
    "# from lincoln.layers import Conv2D\n",
    "\n",
    "from dlfs_kuoste import helpers\n",
    "from dlfs_kuoste import layers\n",
    "from dlfs_kuoste import losses\n",
    "from dlfs_kuoste import optimizers\n",
    "from dlfs_kuoste import operations\n",
    "from dlfs_kuoste import NeuralNetwork\n",
    "from dlfs_kuoste import Trainer\n",
    "\n",
    "X_train, y_train, X_test, y_test = helpers.mnist_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train - np.mean(X_train), X_test - np.mean(X_train)\n",
    "X_train, X_test = X_train / np.std(X_train), X_test / np.std(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_conv, X_test_conv = X_train.reshape(-1, 1, 28, 28), X_test.reshape(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(y_train)\n",
    "train_labels = np.zeros((num_labels, 10))\n",
    "for i in range(num_labels):\n",
    "    train_labels[i][y_train[i]] = 1\n",
    "\n",
    "num_labels = len(y_test)\n",
    "test_labels = np.zeros((num_labels, 10))\n",
    "for i in range(num_labels):\n",
    "    test_labels[i][y_test[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_model(model, test_set):\n",
    "    return print(f'''The model validation accuracy is: \n",
    "    {np.equal(np.argmax(model.forward(test_set, inference=True), axis=1), y_test).sum() * 100.0 / test_set.shape[0]:.2f}%''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 loss 31.191501893742405\n",
      "batch 10 loss 14.150390490525517\n",
      "batch 20 loss 8.507022907684597\n",
      "batch 30 loss 9.81608465956318\n",
      "batch 40 loss 2.7069227167896712\n",
      "batch 50 loss 5.039126892843199\n",
      "batch 60 loss 3.841335350417124\n",
      "batch 70 loss 8.47307919757578\n",
      "batch 80 loss 5.373412749700996\n",
      "batch 90 loss 2.42436564041577\n",
      "batch 100 loss 4.124895016673244\n",
      "Validation accuracy after 100 batches is 85.56%\n",
      "batch 110 loss 9.333326175947795\n",
      "batch 120 loss 5.668763088393043\n",
      "batch 130 loss 8.030682647094096\n",
      "batch 140 loss 1.5587190577707137\n",
      "batch 150 loss 6.860395904028616\n",
      "batch 160 loss 4.220174743632938\n",
      "batch 170 loss 5.190430233727453\n",
      "batch 180 loss 4.674891852621207\n",
      "batch 190 loss 5.2314336031322535\n",
      "batch 200 loss 5.062087142757434\n",
      "Validation accuracy after 200 batches is 85.74%\n",
      "batch 210 loss 3.311997869393835\n",
      "batch 220 loss 5.023688325618068\n",
      "batch 230 loss 4.563027966764308\n",
      "batch 240 loss 3.344317953584475\n",
      "batch 250 loss 3.5711253470995854\n",
      "batch 260 loss 5.177948164842362\n",
      "batch 270 loss 6.217014978569575\n",
      "batch 280 loss 3.763681989585355\n",
      "batch 290 loss 5.945276001554982\n",
      "batch 300 loss 5.112878454672551\n",
      "Validation accuracy after 300 batches is 86.44%\n",
      "batch 310 loss 2.745844832932482\n",
      "batch 320 loss 1.7805313994617051\n",
      "batch 330 loss 4.144722388994526\n",
      "batch 340 loss 1.5403375259284084\n",
      "batch 350 loss 5.527096489278201\n",
      "batch 360 loss 4.594826761771016\n",
      "batch 370 loss 8.507659527402645\n",
      "batch 380 loss 3.196088709150223\n",
      "batch 390 loss 3.6012544189943734\n",
      "batch 400 loss 2.763102123344983\n",
      "Validation accuracy after 400 batches is 89.04%\n",
      "batch 410 loss 1.829048616456381\n",
      "batch 420 loss 4.354178771195306\n",
      "batch 430 loss 4.337995181395511\n",
      "batch 440 loss 4.224254524080593\n",
      "batch 450 loss 5.578514500915521\n",
      "batch 460 loss 4.967396452944138\n",
      "batch 470 loss 5.362821471825029\n",
      "batch 480 loss 5.367537212440094\n",
      "batch 490 loss 3.9230505277697256\n",
      "batch 500 loss 2.0723265950087377\n",
      "Validation accuracy after 500 batches is 87.91%\n",
      "batch 510 loss 4.735917880429457\n",
      "batch 520 loss 3.067803770076915\n",
      "batch 530 loss 4.797460209819897\n",
      "batch 540 loss 3.8179104514452002\n",
      "batch 550 loss 1.3815510666724917\n",
      "batch 560 loss 3.07299878386099\n",
      "batch 570 loss 4.401890938945537\n",
      "batch 580 loss 3.1262011805587635\n",
      "batch 590 loss 1.9493203939603836\n",
      "batch 600 loss 2.918909426222133\n",
      "Validation accuracy after 600 batches is 85.40%\n",
      "batch 610 loss 3.1617284552384914\n",
      "batch 620 loss 4.400817068327972\n",
      "batch 630 loss 5.963283819276061\n",
      "batch 640 loss 3.4176687699287602\n",
      "batch 650 loss 1.3985890819375588\n",
      "batch 660 loss 4.805086778187873\n",
      "batch 670 loss 3.7925894169846113\n",
      "batch 680 loss 7.598582287724296\n",
      "batch 690 loss 2.124792710933418\n",
      "batch 700 loss 2.3505659787487403\n",
      "Validation accuracy after 700 batches is 87.39%\n",
      "batch 710 loss 2.7633121721753287\n",
      "batch 720 loss 2.763102125587555\n",
      "batch 730 loss 5.205842912404834\n",
      "batch 740 loss 1.7453174862626848\n",
      "batch 750 loss 2.0723266311211352\n",
      "batch 760 loss 2.513937008106326\n",
      "batch 770 loss 6.772217216943124\n",
      "batch 780 loss 4.135244797277668\n",
      "batch 790 loss 2.5816517079302983\n",
      "batch 800 loss 1.7101308832723312\n",
      "Validation accuracy after 800 batches is 89.05%\n",
      "batch 810 loss 3.046069489435938\n",
      "batch 820 loss 0.9200884968581418\n",
      "batch 830 loss 1.529975988527912\n",
      "batch 840 loss 3.0321678569944774\n",
      "batch 850 loss 3.599914397943977\n",
      "batch 860 loss 1.5118200542056746\n",
      "batch 870 loss 2.073191296829853\n",
      "batch 880 loss 1.908140299161975\n",
      "batch 890 loss 1.3815510666724917\n",
      "batch 900 loss 2.9466075377561203\n",
      "Validation accuracy after 900 batches is 89.20%\n",
      "batch 910 loss 4.082465807817582\n",
      "batch 920 loss 2.9367749422477702\n",
      "batch 930 loss 0.6907755534639805\n",
      "batch 940 loss 1.5169148940117874\n",
      "batch 950 loss 2.0723265950087377\n",
      "batch 960 loss 2.5092757044778713\n",
      "batch 970 loss 2.7631021233449835\n",
      "batch 980 loss 4.660762983600544\n",
      "batch 990 loss 4.144653310734852\n",
      "Validation loss after 1 epochs is 3.168\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[layers.Conv2D(out_channels=16,\n",
    "                   param_size=5,\n",
    "                   dropout=0.8,\n",
    "                   weight_init=\"glorot\",\n",
    "                   flatten=True,\n",
    "                  activation=operations.Tanh()),\n",
    "            layers.Dense(neurons=10, \n",
    "                  activation=operations.Linear())],\n",
    "            loss = losses.SoftmaxCrossEntropy(), \n",
    "seed=20190402)\n",
    "\n",
    "trainer = Trainer(model, optimizers.SgdMomentum(lr = 0.1, momentum=0.9))\n",
    "trainer.fit(X_train_conv, train_labels, X_test_conv, test_labels,\n",
    "            epochs = 1,\n",
    "            eval_every = 1,\n",
    "            seed=20190402,\n",
    "            batch_size=60,\n",
    "            conv_testing=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model validation accuracy is: \n",
      "    91.47%\n"
     ]
    }
   ],
   "source": [
    "calc_accuracy_model(model, X_test_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearnWin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
